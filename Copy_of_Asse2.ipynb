{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Asse2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM6MT5+Zd8dpqwe/2lNtvNU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yuly4/Pau_and_yuly_Game_Tic_Tac_Toe/blob/master/Copy_of_Asse2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGr0DD2ZFz_E"
      },
      "source": [
        "1. Importing the drive and Loading the data into the data frame. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlVP07S18BZ5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My Drive/cvd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czv4UvEDERZG"
      },
      "source": [
        "2. Import libraries for doing the Exploratory data analysis. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDZsT_4-FES3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import sklearn\n",
        "from scipy import stats\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import recall_score as rs\n",
        "from sklearn.metrics import precision_score as ps\n",
        "from sklearn.metrics import f1_score as fs\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "encoder = preprocessing.LabelEncoder()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfAC2VIOFXAQ"
      },
      "source": [
        "3. Read the csv file, with the correct physical address in the file\n",
        "and Mapping to a Uniform distribution "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ig8TC_rW8Hij"
      },
      "source": [
        "data = pd.read_csv('/content/sample_data/data.csv')\n",
        "data = data.drop('id',axis=1)\n",
        "data = data.fillna(np.nan,axis=0)\n",
        "data['location'] = encoder.fit_transform(data['location'].astype(str))\n",
        "data['country'] = encoder.fit_transform(data['country'].astype(str))\n",
        "data['gender'] = encoder.fit_transform(data['gender'].astype(str))\n",
        "data[['symptom1']] = encoder.fit_transform(data['symptom1'].astype(str))\n",
        "data[['symptom2']] = encoder.fit_transform(data['symptom2'].astype(str))\n",
        "data[['symptom3']] = encoder.fit_transform(data['symptom3'].astype(str))\n",
        "data[['symptom4']] = encoder.fit_transform(data['symptom4'].astype(str))\n",
        "data[['symptom5']] = encoder.fit_transform(data['symptom5'].astype(str))\n",
        "data[['symptom6']] = encoder.fit_transform(data['symptom6'].astype(str))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQSLNfpIHxcR"
      },
      "source": [
        "4. Dropping irrelevants columns (Feautures) \n",
        "*Replace data and transform class types dependencies and Encoding categorical features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrSqYjjf8cJn"
      },
      "source": [
        "data['sym_on'] = pd.to_datetime(data['sym_on'])\n",
        "data['hosp_vis'] = pd.to_datetime(data['hosp_vis'])\n",
        "data['sym_on']= data['sym_on'].map(dt.datetime.toordinal)\n",
        "data['hosp_vis']= data['hosp_vis'].map(dt.datetime.toordinal)\n",
        "data['diff_sym_hos']= data['hosp_vis'] - data['sym_on']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmQag2zgI2Mb"
      },
      "source": [
        "5. Data pre-Processing and model the data, Determine variables of the data set \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy46v0908k0U"
      },
      "source": [
        "data['diff_symp_hos'] = data['hosp_vis']-data['sym_on']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_UXQ0gmJnb4"
      },
      "source": [
        "6. Drop a class of the data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWsc7tCQ8q2-"
      },
      "source": [
        "data = data.drop(['sym_on','hosp_vis'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq3Fhe87KLmZ"
      },
      "source": [
        "7. Show the types of classes are in the data set \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmLn5tGB85kA"
      },
      "source": [
        "print(data.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uni7U960zofH"
      },
      "source": [
        "8. Graph Visualization of the data,using matplotlib library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG-N-aiW9S1E"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def counter2(colname1,colname2):\n",
        "  colname1 = pd.Series(colname1)\n",
        "  colname2 = pd.Series(colname2)\n",
        "  count1 = 0\n",
        "  for i in range(min([colname1.size,colname2.size])):\n",
        "    if(colname1[i]==1 and colname2[i]==1):\n",
        "      count1 = count1+1\n",
        "  return count1\n",
        "\n",
        "def counter1(colname):\n",
        "  colname1 = pd.Series(colname)\n",
        "  count = 0\n",
        "  for i in range(colname1.size):\n",
        "    if(colname1[i]==1):\n",
        "      count = count+1\n",
        "  return count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJrZhld_PA4a"
      },
      "source": [
        "9. Define 2 classes of the data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg-zL-5S9YKV"
      },
      "source": [
        "fwuh = counter1(data['from_wuhan'])\n",
        "vwuh = counter1(data['vis_wuhan'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVLHTgpJPT8Z"
      },
      "source": [
        "10. Show or print the data, with the function of counting per class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvXEfVX59fsj"
      },
      "source": [
        "print(counter1(data['death']))\n",
        "print(counter2(data['from_wuhan'],data['death']))\n",
        "print(counter2(data['vis_wuhan'],data['death']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhumkBO-SAMk"
      },
      "source": [
        "11. Visualization with the matplotlib of the data specific, 2 different classes, 1 label "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sw54bI_C9wn8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.bar(['From Wuhan','Visiting Wuhan'],[counter2(data['death'],data['from_wuhan']),counter2(data['death'],data['vis_wuhan'])],color='green')\n",
        "plt.title('Patient Deaths')\n",
        "plt.xlabel('Patients\\' Native Place')\n",
        "plt.ylabel('Number of Deaths')\n",
        "plt.plot([counter2(data['death'],data['from_wuhan']),counter2(data['death'],data['vis_wuhan'])],color='red')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iQYAcC9S8PD"
      },
      "source": [
        "12. Visualization the data in function 2 types of classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvgNZf7w963T"
      },
      "source": [
        "plt.bar(['From Wuhan','Visiting Wuhan'],[counter2(data['recov'],data['from_wuhan']),counter2(data['recov'],data['vis_wuhan'])],color='purple')\n",
        "plt.title('Recovered Patients')\n",
        "plt.xlabel('Patients\\' Native Place')\n",
        "plt.ylabel('Number of Patients Recovered')\n",
        "plt.plot([counter2(data['recov'],data['from_wuhan']),counter2(data['recov'],data['vis_wuhan'])],color='blue')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlXFIvv3Tp99"
      },
      "source": [
        "13. Training the data \n",
        "* Read the data set from the file train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CqylFOj-Fae"
      },
      "source": [
        "tdata = pd.read_csv('/content/sample_data/train.csv')\n",
        "print(tdata.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ-JyS83WuZA"
      },
      "source": [
        "14. Testing the data \n",
        "*read the data, drop id column, and categorical encoding using label \n",
        " *Mapping to a Uniform distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQXKbwjQ-UkS"
      },
      "source": [
        "tdata = pd.read_csv('/content/sample_data/train.csv')\n",
        "tdata = tdata.drop('id',axis=1)\n",
        "tdata = tdata.fillna(np.nan,axis=0)\n",
        "tdata['age'] = tdata['age'].fillna(value=tdata['age'].mean())\n",
        "tdata['location'] = encoder.fit_transform(tdata['location'].astype(str))\n",
        "tdata['country'] = encoder.fit_transform(tdata['country'].astype(str))\n",
        "tdata['gender'] = encoder.fit_transform(tdata['gender'].astype(str))\n",
        "tdata[['symptom1']] = encoder.fit_transform(tdata['symptom1'].astype(str))\n",
        "tdata[['symptom2']] = encoder.fit_transform(tdata['symptom2'].astype(str))\n",
        "tdata[['symptom3']] = encoder.fit_transform(tdata['symptom3'].astype(str))\n",
        "tdata[['symptom4']] = encoder.fit_transform(tdata['symptom4'].astype(str))\n",
        "tdata[['symptom5']] = encoder.fit_transform(tdata['symptom5'].astype(str))\n",
        "tdata[['symptom6']] = encoder.fit_transform(tdata['symptom6'].astype(str))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN1ZqiSa1VSe"
      },
      "source": [
        "15. Using the library of panda in the specific classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu3R5r2h-jkz"
      },
      "source": [
        "tdata['sym_on'] = pd.to_datetime(tdata['sym_on'])\n",
        "tdata['hosp_vis'] = pd.to_datetime(tdata['hosp_vis'])\n",
        "tdata['sym_on']= tdata['sym_on'].map(dt.datetime.toordinal)\n",
        "tdata['hosp_vis']= tdata['hosp_vis'].map(dt.datetime.toordinal)\n",
        "tdata['diff_sym_hos']= tdata['hosp_vis'] - tdata['sym_on']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwNWKOLo-1dq"
      },
      "source": [
        "16. Drop specific class from the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAyos4Os-8B8"
      },
      "source": [
        "tdata = tdata.drop(['sym_on','hosp_vis'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25W6C2bl-_Tr"
      },
      "source": [
        "17. Show the total data of the training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjCyfZdY_DaU"
      },
      "source": [
        "print(tdata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAG1gA2D_LuV"
      },
      "source": [
        "18. Show the function to detect missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg2CHKsp_Ooa"
      },
      "source": [
        "print(tdata.isna().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-igvYEC8_ehW"
      },
      "source": [
        "19. Import libraries for doing the evaluation metric "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cItk4gII_jJ3"
      },
      "source": [
        "from sklearn.metrics import recall_score as rs\n",
        "from sklearn.metrics import precision_score as ps\n",
        "from sklearn.metrics import f1_score as fs\n",
        "from sklearn.metrics import balanced_accuracy_score as bas\n",
        "from sklearn.metrics import confusion_matrix as cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAv38Xgx_ydR"
      },
      "source": [
        "20. Using the method for classification, Random Forest "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58ORyFqyABZ3"
      },
      "source": [
        "rf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
        "                       criterion='gini', max_depth=2, max_features='auto',\n",
        "                       max_leaf_nodes=None, max_samples=None,\n",
        "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                       min_samples_leaf=2, min_samples_split=2,\n",
        "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
        "                       n_jobs=None, oob_score=False, random_state=None,\n",
        "                       verbose=0, warm_start=False)\n",
        "classifier = AdaBoostClassifier(rf,50,0.01,'SAMME.R',10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iZXQEt_ADUl"
      },
      "source": [
        "21. Boost model of accuracy, Encoding categorical features in x and y "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnyR7J9qAHHf"
      },
      "source": [
        "X = tdata[['location','country','gender','age','vis_wuhan','from_wuhan','symptom1','symptom2','symptom3','symptom4','symptom5','symptom6','diff_sym_hos']]\n",
        "Y = tdata['death']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTu-JDu_AoVb"
      },
      "source": [
        "22. Train the model to the data set, with the library of sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUVVa13gAt-N"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2,random_state=0)\n",
        "classifier.fit(X_train,np.array(Y_train).reshape(Y_train.shape[0],1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zwcld7YA9CN"
      },
      "source": [
        "23. Do the evaluation metrics, based of the traini g model, and apply the confusion matrix, for get TP,TN,FP AND FN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eNcxtODBFik"
      },
      "source": [
        "pred = np.array(classifier.predict(X_test))\n",
        "\n",
        "recall = rs(Y_test,pred)\n",
        "precision = ps(Y_test,pred)\n",
        "f1 = fs(Y_test,pred)\n",
        "ma = classifier.score(X_test,Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ynrma_fLBcGy"
      },
      "source": [
        "24. Show the Evaluation metrics for test dataset / Doing the evaluation with the libraries, in this step is important to run all or the last 2 to do the evaluation. \n",
        "*Show Precision Score, F1 Score and Accuracy  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2Kpkw3PBw1e"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "sklearn.metrics.recall_score\n",
        "\n",
        "print('Precision Score: ',[precision])\n",
        "print('F1 Score: ',f1)\n",
        "print('Accuracy: ',ma)\n",
        "a = pd.DataFrame(Y_test)\n",
        "a['pred']= classifier.predict(X_test)\n",
        "print('\\n\\tTable 3\\n')\n",
        "print(a.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URtb5dGUEDg-"
      },
      "source": [
        "26. Show the values of the method of classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAsbqxf9EHZr"
      },
      "source": [
        "print(pd.DataFrame({'Val':Y_test,'Pred':classifier.predict(X_test)}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJUeC4TnETOB"
      },
      "source": [
        "27. Using Random Forest for get the estimation in max and min"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AIx7q7YEZJ3"
      },
      "source": [
        "X1 = tdata[['location','country','gender','age','vis_wuhan','from_wuhan','symptom1','symptom2','symptom3','symptom4','symptom5','symptom6','diff_sym_hos']]\n",
        "Y1 = tdata['death']\n",
        "classifier1 = RandomForestClassifier()\n",
        "\n",
        "n_estimators = [100,200,300,400,500]\n",
        "max_depth = [1,2,5,6]\n",
        "min_samples_split = [1,2,6,7]\n",
        "min_samples_leaf = [2,3,4,5]\n",
        "\n",
        "params_grid = {'n_estimators':n_estimators,'max_depth':max_depth,'min_samples_split':min_samples_split,'min_samples_leaf':min_samples_leaf}\n",
        "\n",
        "gridder = GridSearchCV(estimator=classifier1,param_grid=params_grid,n_jobs=-1,cv=5,verbose=5 )\n",
        "gridder.fit(X1,np.array(Y1).reshape(Y1.shape[0],1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtVE48vQE_8n"
      },
      "source": [
        "28. Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orGyOKwPFCuU"
      },
      "source": [
        "print(gridder.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9fzJB4pOP4R"
      },
      "source": [
        "29. Watch current working directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReV7VIKVOR6s"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY0DhspvOtOa"
      },
      "source": [
        "30. Read the data set, from the file. \n",
        "*Drop a category of the data set call ID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlSV3vCwO11d"
      },
      "source": [
        "udata = pd.read_excel('/content/sample_data/test.xlsx')\n",
        "udata = udata.drop('id',axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YL451WPPLtr"
      },
      "source": [
        "31.Show the update columns (Features) from the data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvHpbto4Pc9L"
      },
      "source": [
        "print(udata.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt8477m0PqE9"
      },
      "source": [
        "32. Pre Processing the data, and using the trasform method on the classes\n",
        "*Mapping to a Uniform distribution "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q_ARivzPwQX"
      },
      "source": [
        "udata = udata.fillna(np.nan,axis=0)\n",
        "udata['age'] = udata['age'].fillna(value=udata['age'].mean())\n",
        "udata['from_wuhan'] = udata['from_wuhan'].fillna(value=0)\n",
        "udata['from_wuhan'] = udata['from_wuhan'].astype(int)\n",
        "udata['location'] = encoder.fit_transform(udata['location'].astype(str))\n",
        "udata['country'] = encoder.fit_transform(udata['country'].astype(str))\n",
        "udata['gender'] = encoder.fit_transform(udata['gender'].astype(str))\n",
        "udata[['symptom1']] = encoder.fit_transform(udata['symptom1'].astype(str))\n",
        "udata[['symptom2']] = encoder.fit_transform(udata['symptom2'].astype(str))\n",
        "udata[['symptom3']] = encoder.fit_transform(udata['symptom3'].astype(str))\n",
        "udata[['symptom4']] = encoder.fit_transform(udata['symptom4'].astype(str))\n",
        "udata[['symptom5']] = encoder.fit_transform(udata['symptom5'].astype(str))\n",
        "udata[['symptom6']] = encoder.fit_transform(udata['symptom6'].astype(str))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo-37QrmQBdC"
      },
      "source": [
        "33. Show the data set of the specific class\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7xbrBPnQN1o"
      },
      "source": [
        "print(udata['from_wuhan'].mode())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Is7ISAAQTZK"
      },
      "source": [
        "34. Using pandas library in to determine the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlahzE6fQV2-"
      },
      "source": [
        "udata['sym_on'] = pd.to_datetime(udata['sym_on'])\n",
        "udata['hosp_vis'] = pd.to_datetime(udata['hosp_vis'])\n",
        "udata['sym_on']= udata['sym_on'].map(dt.datetime.toordinal)\n",
        "udata['hosp_vis']= udata['hosp_vis'].map(dt.datetime.toordinal)\n",
        "udata['diff_sym_hos']= udata['hosp_vis'] - udata['sym_on']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wex0Qc1bQkDz"
      },
      "source": [
        "35. Show the data with the method unique"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At7ia_CRQlbw"
      },
      "source": [
        "print(udata['from_wuhan'].unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvOK9nmpQvlB"
      },
      "source": [
        "36. Show the update data types of the data set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZcfs5I8Qzru"
      },
      "source": [
        "print(udata.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekMKyqzkRAG7"
      },
      "source": [
        "37. classification of the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1lFHxKIROsj"
      },
      "source": [
        "udata = udata[['location','country','gender','age','vis_wuhan','from_wuhan','symptom1','symptom2','symptom3','symptom4','symptom5','symptom6','diff_sym_hos']]\n",
        "udata['result'] = classifier.predict(udata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqRkJDZW03XC"
      },
      "source": [
        "38. Show the result in the feature (result) of the data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTtvXJms1Idc"
      },
      "source": [
        "print(udata['result'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElMOgaep1OaU"
      },
      "source": [
        "39. Import a particular drive with file on cvd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMGrbfmX1TS8"
      },
      "source": [
        "!cd '/gdrive/My Drive/cvd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LznXxLCs2Ozm"
      },
      "source": [
        "40. Read external data from the drive, following the exact address where is the file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2gH-0Ds2VKC"
      },
      "source": [
        "udata.to_csv('/gdrive/My Drive/final.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}